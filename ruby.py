# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16vj88PWa6nwERSDYJpwug_YlIW4NcSrC
"""

import gradio as gr
import PyPDF2
import io
import logging
import traceback
from typing import List, Tuple, Optional
from dataclasses import dataclass
from enum import Enum
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import re
import os
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(_name_)   # ‚úÖ FIXED

class ErrorType(Enum):
    PDF_PROCESSING = "PDF Processing Error"
    MODEL_LOADING = "Model Loading Error"
    MODEL_INFERENCE = "Model Inference Error"
    VALIDATION = "Input Validation Error"
    SYSTEM = "System Error"
    NETWORK = "Network Error"

@dataclass
class StudyMateError:
    error_type: ErrorType
    message: str
    technical_details: str = ""
    user_message: str = ""
    timestamp: str = ""

    def _post_init(self):   # ‚úÖ FIXED (was _post_init)
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()
        if not self.user_message:
            self.user_message = self._generate_user_message()

    def _generate_user_message(self) -> str:
        """Generate user-friendly error messages"""
        user_messages = {
            ErrorType.PDF_PROCESSING: "‚ùå Unable to process the PDF file. Please ensure it's a valid PDF with readable text.",
            ErrorType.MODEL_LOADING: "ü§ñ AI model is temporarily unavailable. Please try again in a moment.",
            ErrorType.MODEL_INFERENCE: "üí≠ Unable to generate response. Please try rephrasing your question.",
            ErrorType.VALIDATION: "‚ö† Please check your input and try again.",
            ErrorType.SYSTEM: "üîß A system error occurred. Please try again.",
            ErrorType.NETWORK: "üåê Network connection issue. Please check your connection and retry."
        }
        return user_messages.get(self.error_type, "An unexpected error occurred.")

class ErrorHandler:
    def _init(self):   # ‚úÖ FIXED (was _init)
        self.error_log = []

    def handle_error(self, error: Exception, error_type: ErrorType,
                    context: str = "", user_message: str = "") -> StudyMateError:
        """Central error handling with logging and user-friendly messages"""
        technical_details = f"{context}\nException: {str(error)}\nTraceback: {traceback.format_exc()}"

        study_error = StudyMateError(
            error_type=error_type,
            message=str(error),
            technical_details=technical_details,
            user_message=user_message
        )

        # Log error
        logger.error(f"{error_type.value}: {study_error.message}")
        logger.debug(f"Technical details: {technical_details}")

        # Store for debugging
        self.error_log.append(study_error)

        return study_error

    def get_error_summary(self) -> str:
        """Get summary of recent errors for debugging"""
        if not self.error_log:
            return "No errors recorded."

        recent_errors = self.error_log[-5:]  # Last 5 errors
        summary = "Recent Errors:\n"
        for i, error in enumerate(recent_errors, 1):
            summary += f"{i}. {error.timestamp}: {error.error_type.value} - {error.message}\n"
        return summary

class StudyMate:
    def _init(self):   # ‚úÖ FIXED (was _init)
        self.error_handler = ErrorHandler()
        self.tokenizer = None
        self.model = None
        self.pdf_content = ""
        self.is_model_loaded = False

        # Initialize model on startup
        self._load_model()

    def _load_model(self):
        """Load IBM Granite model with comprehensive error handling"""
        try:
            logger.info("Loading IBM Granite model...")

            # Check if GPU is available
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"Using device: {device}")

            self.tokenizer = AutoTokenizer.from_pretrained(
                "ibm-granite/granite-3.3-2b-instruct",
                trust_remote_code=True
            )

            self.model = AutoModelForCausalLM.from_pretrained(
                "ibm-granite/granite-3.3-2b-instruct",
                trust_remote_code=True,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None
            )

            if device == "cpu":
                self.model = self.model.to(device)

            self.is_model_loaded = True
            logger.info("Model loaded successfully")

        except Exception as e:
            error = self.error_handler.handle_error(
                e, ErrorType.MODEL_LOADING,
                "Failed to load IBM Granite model",
                "ü§ñ AI model failed to load. Please restart the application."
            )
            self.is_model_loaded = False
            raise Exception(error.user_message)

    # ‚úÖ Rest of your methods (process_pdf, answer_question, etc.)
    # No changes needed except fixing init and name.

# Initialize StudyMate
try:
    study_mate = StudyMate()
    initialization_message = "‚úÖ StudyMate initialized successfully! Upload a PDF to get started."
except Exception as e:
    study_mate = None
    initialization_message = f"‚ùå Failed to initialize StudyMate: {str(e)}"

# Gradio Interface
def create_interface():
    """Create the Gradio interface with error handling"""
    with gr.Blocks(theme=gr.themes.Soft(), title="StudyMate: AI-Powered PDF Q&A") as interface:
        gr.HTML("<h1>üìö StudyMate: AI-Powered PDF Q&A</h1>")
        status_display = gr.Textbox(value=initialization_message, label="üìä Status", interactive=False)
        # keep the rest of your UI code here...
    return interface

# Launch the application
if _name_ == "_main":   # ‚úÖ FIXED (was _name)
    try:
        interface = create_interface()
        interface.launch(server_name="0.0.0.0", server_port=7860, share=False, debug=False)
    except Exception as e:
        logger.error(f"Failed to launch application: {e}")
        print(f"‚ùå Application failed to start: {str(e)}")

!pip install gradio transformers sentence-transformers faiss-cpu PyMuPDF torch

import gradio as gr
import PyPDF2
import io
import logging
import traceback
from typing import List, Tuple, Optional
from dataclasses import dataclass
from enum import Enum
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import re
import os
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)   # ‚úÖ FIXED

class ErrorType(Enum):
    PDF_PROCESSING = "PDF Processing Error"
    MODEL_LOADING = "Model Loading Error"
    MODEL_INFERENCE = "Model Inference Error"
    VALIDATION = "Input Validation Error"
    SYSTEM = "System Error"
    NETWORK = "Network Error"

@dataclass
class StudyMateError:
    error_type: ErrorType
    message: str
    technical_details: str = ""
    user_message: str = ""
    timestamp: str = ""

    def __post_init__(self):   # ‚úÖ FIXED
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()
        if not self.user_message:
            self.user_message = self._generate_user_message()

    def _generate_user_message(self) -> str:
        """Generate user-friendly error messages"""
        user_messages = {
            ErrorType.PDF_PROCESSING: "‚ùå Unable to process the PDF file. Please ensure it's a valid PDF with readable text.",
            ErrorType.MODEL_LOADING: "ü§ñ AI model is temporarily unavailable. Please try again in a moment.",
            ErrorType.MODEL_INFERENCE: "üí≠ Unable to generate response. Please try rephrasing your question.",
            ErrorType.VALIDATION: "‚ö† Please check your input and try again.",
            ErrorType.SYSTEM: "üîß A system error occurred. Please try again.",
            ErrorType.NETWORK: "üåê Network connection issue. Please check your connection and retry."
        }
        return user_messages.get(self.error_type, "An unexpected error occurred.")

class ErrorHandler:
    def __init__(self):   # ‚úÖ FIXED
        self.error_log = []

    def handle_error(self, error: Exception, error_type: ErrorType,
                    context: str = "", user_message: str = "") -> StudyMateError:
        """Central error handling with logging and user-friendly messages"""
        technical_details = f"{context}\nException: {str(error)}\nTraceback: {traceback.format_exc()}"

        study_error = StudyMateError(
            error_type=error_type,
            message=str(error),
            technical_details=technical_details,
            user_message=user_message
        )

        # Log error
        logger.error(f"{error_type.value}: {study_error.message}")
        logger.debug(f"Technical details: {technical_details}")

        # Store for debugging
        self.error_log.append(study_error)

        return study_error

    def get_error_summary(self) -> str:
        """Get summary of recent errors for debugging"""
        if not self.error_log:
            return "No errors recorded."

        recent_errors = self.error_log[-5:]  # Last 5 errors
        summary = "Recent Errors:\n"
        for i, error in enumerate(recent_errors, 1):
            summary += f"{i}. {error.timestamp}: {error.error_type.value} - {error.message}\n"
        return summary

class StudyMate:
    def __init__(self):   # ‚úÖ FIXED
        self.error_handler = ErrorHandler()
        self.tokenizer = None
        self.model = None
        self.pdf_content = ""
        self.is_model_loaded = False

        # Initialize model on startup
        self._load_model()

    def _load_model(self):
        """Load IBM Granite model with comprehensive error handling"""
        try:
            logger.info("Loading IBM Granite model...")

            # Check if GPU is available
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"Using device: {device}")

            self.tokenizer = AutoTokenizer.from_pretrained(
                "ibm-granite/granite-3.3-2b-instruct",
                trust_remote_code=True
            )

            self.model = AutoModelForCausalLM.from_pretrained(
                "ibm-granite/granite-3.3-2b-instruct",
                trust_remote_code=True,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None
            )

            if device == "cpu":
                self.model = self.model.to(device)

            self.is_model_loaded = True
            logger.info("Model loaded successfully")

        except Exception as e:
            error = self.error_handler.handle_error(
                e, ErrorType.MODEL_LOADING,
                "Failed to load IBM Granite model",
                "ü§ñ AI model failed to load. Please restart the application."
            )
            self.is_model_loaded = False
            raise Exception(error.user_message)

    # TODO: add methods like process_pdf(), answer_question(), etc.


# Initialize StudyMate
try:
    study_mate = StudyMate()
    initialization_message = "‚úÖ StudyMate initialized successfully! Upload a PDF to get started."
except Exception as e:
    study_mate = None
    initialization_message = f"‚ùå Failed to initialize StudyMate: {str(e)}"


# Gradio Interface
def create_interface():
    """Create the Gradio interface with error handling"""
    with gr.Blocks(theme=gr.themes.Soft(), title="StudyMate: AI-Powered PDF Q&A") as interface:
        gr.HTML("<h1>üìö StudyMate: AI-Powered PDF Q&A</h1>")
        status_display = gr.Textbox(value=initialization_message, label="üìä Status", interactive=False)

        # ‚úÖ Use filepath instead of file
        pdf_upload = gr.File(label="Upload PDF", file_types=[".pdf"], type="filepath")

        # (Placeholder for Q&A UI components)
        question = gr.Textbox(label="Ask a Question")
        answer = gr.Textbox(label="Answer")

    return interface


# Launch the application
if __name__ == "__main__":   # ‚úÖ FIXED
    try:
        interface = create_interface()
        interface.launch(server_name="0.0.0.0", server_port=7860, share=False, debug=False)
    except Exception as e:
        logger.error(f"Failed to launch application: {e}")
        print(f"‚ùå Application failed to start: {str(e)}")